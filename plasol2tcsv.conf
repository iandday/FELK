#/opt/logstash/bin/logstash -t -f /etc/logstash/conf.d/

input {
  file {
    path => ["/usr/local/logstashInput/plaso/*/*.csv"]
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}

filter {
    csv {
        columns => ["date","time","timezone","MACB","source","sourcetype","type","user","host","short","desc","version","filename","inode","notes","format","extra"]
              separator => ","
              quote_char => "Âª"       # workaround: don't use a quote character as " gives issues if the field contains a "
    }
    if [date] == "date" {
       drop {}  # drop the first line that contains the column names
    }
    mutate { merge => ["date", "time"] }       # merge and join need to be in separate mutates
    mutate { merge => ["date", "timezone"] }   # merge and join need to be in separate mutates
    mutate { join => ["date", " "] }           # merge and join need to be in separate mutates
    date {
      match => ["date", "MM/dd/YYYY HH:mm:ss z" ]
    }

    #extract MACB info
    if ("M" in [MACB]) { mutate { add_tag => ["modified"] } }
    if ("A" in [MACB]) { mutate { add_tag => ["accessed"] } }
    if ("C" in [MACB]) { mutate { add_tag => ["changed"] } }
    if ("B" in [MACB]) { mutate { add_tag => ["birth"] } }

    #Extract filenames
    if [source] == "FILE" {
                #break multiple paths from desc field
                  ruby {
                         code => "
                                fieldArray = event['desc'].split(';')
                                i = 1
                                for field in fieldArray
                                  field.sub!('TSK:','')
                                  event['extractedPath' + i.to_s] = File.dirname(field)
                                  if File.dirname(field).match(/VSS\d:.*/)
                                        (event['tags'] ||= []) << 'inVSC' unless event['tags'].include? 'inVSC'
                                  end
                                  event['extractedFileName' + i.to_s] = File.basename(field).split('.').first
                                  event['extractedFileExt' + i.to_s] = File.extname(field)[1..-1]
                                  i += 1
                                end
                                        "
                                }
        }
    if [source] == "META" {
      grok {
        break_on_match => false
        match => ["filename", "(:(?<extractedPath>/.*?))?$",
                  "extractedPath", "(?<extractedFilename>[^/]+?)?$",
                  "extractedFilename", "((\.(?<extractedExt>[^./]+))?)?$"
                 ]
      }
    }
	
	if [source] == "EVT" {
			if [sourcetype] == "WinEVTX" {
			#	grok {
			#		match => ["desc", "\A\[%{NUMBER:eventid} / %{BASE16NUM:eventidhex}] Record Number: #%{BASE10NUM:recordnumber} Event Level: %{BASE10NUM:eventlevel} Source Name: %{NOTSPACE:sourcename} #Computer Name: %{NOTSPACE:computername} Message string: %{CISCO_REASON:message}"]
			#	}
			grok {
					match => ["desc", "\A\[%{BASE10NUM:eventid} / %{BASE16NUM:eventidhex}] %{GREEDYDATA:eventdesc}"]
				}
			
			}
			#filter{
			#	xml{
			#	source => "extra"
			#	target => "message_parsed"
			#	add_tag => ["xml_parsed"]
			#	}
			#}
		}
    #Extract urls
    if [source] == "WEBHIST" {
		if "Original URL:" in [desc]   {
			grok { match => ["desc", "Original URL: %{GREEDYDATA:extractedURL}"] }
		}
		else if "URL:" in [desc] {
			grok { match => ["desc", "URL: (?<extractedURL>.*?)[ $]"] }
		}
		else if "Location:" in [desc]  {
			grok { match => ["desc", "Location: (?<extractedURL>.*?)[ $]"] }
		}
    }
    mutate {
      convert => ["inode", "integer",
                  "version", "integer"]
      lowercase => ["extractedExt"]
      remove_field => ["message", "short", "date", "time", "timezone"]
    }
  #Add field for filename
 grok {
        match => [ "path", "/usr/local/logstashInput/plaso/(?<caseNum>[^/]+)/(?<evidenceFile>.*).csv" ]
     }
  }


output {
    elasticsearch {
    action => "index"
    index => "logstash-plasocsv"
    host => "localhost"
   }
  }
